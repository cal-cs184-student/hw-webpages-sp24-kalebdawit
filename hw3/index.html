<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>CS 184 HW 3 Write-Up</title>
</head>
<body>
	<h1>CS 184 HW 3 Write-Up</h1>
	<h2>Overview</h2>
    <p>This project was quite the journey. I implemented ray generation and scene intersection to allow us to draw rays from our camera sensor and view the world with perspective. Next, I built a BVH in order to perform primitive intersection and drawing with greater render speed. Next, I stepped through direct and global illuminance to provide better light path capturing. Finally, I gave adaptive sampling an attempt to find a balance between noise reduction and render speed.</p>
	<h2>Part 1: Ray Generation and Scene Intersection</h2>
    <p>In ray generation, we take an image coordinate and create a ray that shoots out to our scene in world coordinates. To do this, I first converted the coordinates in image space into the corresponding location in camera sensor space my leveraging the vertical and horizontal field of view of our lens. Next, I generated a ray vector in this sensor space and used it to build a ray in world space. Finally, I set clipping values for the ray in order to allow for range control later in the project. For triangle intersection, I used the Moller-Trumbore algorithm to determine the barycentric coordinates and the intersection t-value. Using this, I could check if the ray intersected the triangle within the range limits of the ray. Additionally, I can check if the barycentric coordinates are within the bounds of 0 to 1, to see if the plane intersection point is within our desired triangle primitive. Nested in this algorithm, is the check to see if the ray intersects the plane that the triangle is in, and then we find if the triangle exactly was intersected, and if so, the intersection point / interpolated normal. For sphere intersection, I used the quadratic formula to parametrize the equation of the sphere. Using this, I could easily find the roots and determine the entrance and exit points of the sphere.</p>
	<img src="hw3/part1.png" alt="Image 1">
	<h2>Part 2: BVH</h2>
	<p>When constructing the BVH, I first expanded the BVHNode bounding box to contain the bounding boxes of all primitives inside of it. Next, I checked if the number of primitives at the current node was small enough to create a leaf. If the number of primitives was less than the max leaf size, then I set the node's start and end iterators and returned it. If it wasn't, then I split the primitives into even groups on a selected axis. My heuristic to decide the split axis is to determine the bounding box centroid axis with the greatest variance across the axis mean. My intuition was that the axis with the greatest variance along it would produce the most information gain if I split on it. Once I split along the axis into two groups, I recursively calculated the sub-BVH at each of these nodes and assigned the resulting nodes to be the left and right child nodes of the current BVHNode.</p>
	<img src="hw3/part2a.png" alt="Image 2">
	<img src="hw3/part2b.png" alt="Image 3">
	<p>Based on the renders above, using a BVH to search for primitive intersections greatly increased the render time. Rendering the cow sped up from 21 seconds to 0.1 seconds. Rendering maxplanck sped up from 214 seconds to 0.19 seconds. Rendering the dragon sped up from 478 seconds to 0.14 seconds. Something that I find interesting is that despite changes in the original rendering time of the images, the rendering time when using a bounding volume hierarchy is relatively consistent. This shows that our model is fairly resistant to image complexity and does well to break down the rendering problem into easier subproblems.</p>
	<h2>Part 3: Direct Illumination</h2>
	<p>In Part 3, I implemented direct illumination with hemisphere sampling and light importance sampling. For each sample in hemisphere sampling, I sampled a light input vector from a uniformly distributed hemisphere sampler. Then, I converted this sampled vector into the world space and generated a ray from the hit point in the vector direction to search for an input light source. If the primitive that the ray intersects had no emission (not a light source), then I would skip to the next sample. However, if our search ray hit a light source, then the algorithm weights the source emission by the cosine of the angle between the sampled input vector and the hit point normal. This is then scaled by the BSDF value mapping from our sampled input vector to our output vector and normalized by the hemisphere PDF (1/2pi). This sample reflectance is then added to our reflectance sum. After all samples are collected, then this sum is divided by the number of samples to output an average reflectance.</p>
	<p>For direct illumination through light importance sampling, we reduce noise by directly sampling the lights in a scene rather than randomly sampling a uniform hemisphere. For each light source, we sample the light source area and perform a similar reflectance calculation process as in hemisphere sampling illumination. We draw a shadow ray from our hit point to the sample direction towards the light source. If we intersect an object along the way, then we know this sample is a shadow and we don't add anything to our reflectance sum. If the ray doesn't intersect another primitive on the way to the light source, then we know that the light source reaches the hit point for that sample, and we perform the same reflectance summation and averaging as described in the previous paragraph.</p>
	<img src="hw3/part3a.png" alt="Image 4">
	<img src="hw3/part3b.png" alt="Image 5">
	<img src="hw3/part3c.png" alt="Image 6">
	<img src="hw3/part3d.png" alt="Image 7">
	<img src="hw3/part3e.png" alt="Image 8">
	<p>The clear difference between our direct light images that leverage uniform hemisphere sampling and the images that use light importance sampling is that the images that use light sampling are much less noisy. As the images above display, we have sharper lines and less scattered black dots. This is because we don't have to randomly project rays in the hopes that we get meaningful outcomes. With light importance sampling, we are localizing our input samples to relevant light sources. Furthermore, as we increase the number of light rays that we use, we can get very good results without having to take that many samples per pixel. The last bunny image above was sampled at a rate of 1 sample per pixel with 64 light rays used, and it was quite clear. Additionally, it rendered pretty fast.</p>
	<h2>Part 4: Global Illumination</h2>
	<p>I implemented global illumination through a recursive bounce radiance algorithm. The base case is if we are at are last bounce; in this case, we simply return the emission at the current intersection. If we're not at this base case, then we sample the current intersection's BSDF with the current output vector in order to get our sampled input light ray vector and the corresponding PDF. With this sample, we can create a ray that searches for our next bounce (in inverse order of the original light path). If our search ray intersects the scene, then we can recursively calculate the rest of the bounce path and scale it by our BSDF f-value and input angle, along with normalizing by our PDF. Along the recursive path, we add up the illumination at each bounce to get a rendering solution that takes both direct and indirect lighting into account.</p>
	<img src="hw3/part4a.png" alt="Image 9">
	<img src="hw3/part4b.png" alt="Image 10">
	<img src="hw3/part4c.png" alt="Image 11">
	<img src="hw3/part4d.png" alt="Image 12">
	<img src="hw3/part4e.png" alt="Image 13">
	<img src="hw3/part4f.png" alt="Image 14">
	<img src="hw3/part4g.png" alt="Image 15">
	<p>My debugging journey here was quite difficult. At first, I was running into a bug, where my code would run into an infinite loop and just stall instead of flipping the edge. This loop occurred after my entire method ran and returned, which made it even more confusing. I realized that this bug was because I created a new edge in the mesh instead of just updating the existing pointer. After I fixed this, I ran into another bug where whenever I tried to flip an edge, the two adjacent triangles would disappear. I eventually resorted to assigning variables for every edge and setting new neighbors for all of them as I described above. This ended up being the cleanest solution and working perfectly.</p>
	<h2>Part 5: Adaptive Sampling</h2>
	<p>Adaptive sampling is a clever way for us to find a balance between reducing noise and speeding up rendering times. Essentially, we adjust the sampling rate in di]erent regions of the image. Specifically, we take more samples per pixel in areas of the image that require higher resolution to reduce noise, while taking less samples in areas that are less visually significant. To do this, we evaluate when a given pixel has "converged" on a good enough value and further samples don't change it much. In my implementation, I kept a counter of how many samples I've taken for each pixel. Then I would check on if the pixel samples have converged after every batch (n % samplesPerBatch == 0). If they did based on the rule below, then I would exit the sample and store the current average value.</p>
	<img src="hw3/part5a.png" alt="Image 16">
	<p>Below are my results when looking at the rate images of my adaptive sampling. For some reason, I was getting some funky renders. After hours of debugging, I realized my running sum float values were disappearing at some point and only getting checked when they hit 0. I fiddled around with it for a while but couldn't figure out a solution. I also included some runs where I just used one bounce and it seemed more like what I should expect, leading me to believe that maybe this issue could trace to how I'm processing my global illumination methods. Who knows? It was a pain to debug, but I learned a lot from this project.</p>
	<img src="hw3/part5b.png" alt="Image 17">
</body>
</html>